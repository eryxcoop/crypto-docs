# Introduction to Zero Knowledge Proofs
Disclaimer: this is intended as a first approximation to ZK, most details are not covered and many simplifications have been done for educational purposes.

## Introduction
Given a hash function $H$ and input $x$, it is easy to obtain $H(x)=y$. But given $y$, it is hard to find an $x$ such that $H(x)=y$. It is difficult to find the preimage of a hash.

Hashes are used in the context of blockchain to prove you own something. For example, someone can put 100 raccoon-coins in a lockbox and say *"Anyone who proves he knows $x$ such that $H(x)=y$, can take this raccoon-coins"*. Suppose you know $x$. Great! You can claim this juicy coins. But what happens if you want to keep $x$ secret to avoid someone else using $x$ for future claims?

If would be nice if you could run the following program privately in your computer:

```python
def claim_raccoon_coins(x):
    y = "0xFF081CAFE"
    return H(x) == y
```

And generate a proof that convinces others that you executed that particular program and it returned `True`, without revealing your secret $x$.

Zero knowledge proving systems consist of a prover $P$ and a verifier $V$. First, the prover and the verifier agree on some specific computer program. Then the prover sends to the verifier a proof $\pi$ and a result $r$. This proof convinces $V$ that $P$ has executed the agreed program and it returned $r$. It is called a *zero knowledge* proof because it does not reveal any sensitive information. The prover can use secret input parameters and the values of the local variables are not revealed to the verifier. The verifier only knows the instructions and constants, that is, the "text" of the agreed program.

It is desirable that the proof size is small and easy to verify. This property is called **succinctness**. A common term used in ZK is **zkSNARK**, this stands for zero knowledge succint argument of knowledge.

## General strategy
At first it is hard to image how can $V$ be convinced. Let's break down how a ZK proving system works. There are two main components:
- Arithmetization
- Polynomial commitment scheme

Both of these come in different flavours and end up affecting the performance of the system and the proof size. Let's analyze this two components separately and then see how they fit together to make a proof.

> *Mathematical note*: when refering to variables $x$, $y$, they will be elements of a **finite field** $\mathbb{F}$ except stated otherwise. This finite field is agreed between $P$ and $V$. Understanding finite fields is beyond the scope of this text. Also a **domain** over this finite field is agreed between the prover and the verifier. This domain is generated by a root of unity $\omega$.

# Arithmetization
The goal of this process is to start thinking about the program mathematically. We can think of a program as having two important things:
- *Memory*: a place to store intermediate steps of its computation. It can be seen as a vector $\vec{w}$ that stores values.
- *Logic*: it can be seen as a system of polynomial constraints $R(x)$, that describe which $\vec{w}$ are valid executions.

Not every memory vector $\vec{x}$ will correspond to the exepcted logic of the program. If the constraints are zero, then the variables in $\vec{w}$ have the relationship determined by the program:

$$R(\vec{w})=0 \iff \vec{w} \text{ follows the expected logic of the program}$$


In zero-knowledge the logic of the program encoded in $R$ is known for both the prover and the verifier, while the content of the variables, encoded in $\vec{w}$ is only known to the prover who has run the computer program.

This looks confusing at first. Let's see examples of how $R$ and $w$ look like. There are two different strategies to intuetivily come up with them.

## Circuits
The first strategy is to think about the program as a "hardware circuit". This circuit has sum and multiplication gates. Each gate has inputs and outputs. For example, a sum gate has two inputs `(x, y)` and one output `(z)`. The sum gate enforces the constraint `x + y == z`. Combining several sum and multiplication gates you end up basically with a complex system of constraints $R(x)$:

![image](https://hackmd.io/_uploads/r1tpHA6u6.png)

To obtain a feasible solution $\vec{w}$ you can compute the circuit gates. A disadvantage is that circuits are **rigid**. If you want to prove different programs (other than `claim_raccoon_coins`), you have to design a different circuit for each of them. Also, using gates is too low-level. There are tools to recycle common circuits called **gadgets**.

## Execution trace
Another strategy is to design a virtual machine. A virtual machine essentially has a set of registers to store values and a set of instructions that describe valid transitions from a state to the following.

> ### Small example
> When executing a VM we obtain a series of states. This sequence of states is what we call an **execution trace**. Here we have the trace of a simple VM that was executed for 3 steps:
> 
> | OP  |  A       | B        | C        |
> | ----| -------- | -------- | -------- |
> |  1  |  1       | 1        | 2        |
> |  0  |  3       | 2        | 8        |
> |  1  |  3       | 0        | 11       |
>
> This machine is very simple. **OP** is a register that stores opcodes. **A** and **B** store operands, and **C** stores the cumulative results (starting at zero). If OP == 1 then A + B is added to C. If OP == 0 then A * B is added to C.
> If we see $op$, $a$, $b$ and $c$ as polynomials such that $$op(\omega^i)=OP_i \ ; \ a(\omega^i)=A_i \ ; \ b(\omega^i)=B_i \ ; \ c(\omega^i)=C_i$$
> over the domain $\omega^i$, then this can be expressed as a system of equations $R(x)$:
>  
> **Boundary constraints**:
> $c(\omega^0)=0$
> 
> **Transition constraints**:
> $c(\omega x) = op(x) * (a(x) + b(x) + c(x)) + (1 - op(x)) * (a(x) * b(x) + c(x))$ 
> 
> $a, b$ and $c$ can be easily obtained through interpolation. Here, the witness $w$ can be $c(x)$.

The advantage of this strategy is **flexibility**. You can code different programs with the VM and they will be accepted. You don't need to design different circuits for different programs. The disadvantage is that you may have **performance overhead** compared to circuits.

## Common practices in constraints
The goal of circuits and execution traces is to intuitively derive the constraints $R(x)$ that describe computation. But, we can add any additional constraints if we find it useful. Here're some frequent practices in proving systems.

### Selectors
An example of a selector is the **OP** column in the execution trace example. Here, a polynomial $op(x)$ "selects" which operation is chosen (add or mul). This type of selector polynomials appear frequently inside constraints.

Example: 
$$
s(x) A + (1-s(x)) B = C
$$

The selector $s(x)$ defines whether $A$ or $B$ will be equal to $C$.,

## Nothing matters
Although this two arithmetization strategies (circuits and execution trace) may seem different, note that in the end both just end up being a system of polynomial constraints $R(x)$. That is good, because we need polynomials for our the next step: the polynomial commitment scheme.

Note: This system of constraints is sometimes called **R1CS** (Rank one constraint system).

# Polynomial commitment scheme
In this protocol, the prover commits to a polynomial $p$ without revealing any information about $p$ to $V$. We say a PCS is **binding** because the prover can't change the polynomial $p$ once he has commited to it (otherwise $V$ would notice). A commitment is also **hiding** because it doesn't reveal unintended information about $p$ to $V$.

A normal exchange between $P$ and $V$ would include three operations called $\text{commit}$, $\text{open}$ and $\text{verify}$. Suppose $P$ and $V$ want to have an interaction over some polynomial $t(x)$:
- `[T] := commit(t)`: the prover computes $[T]$, the commitment of $t(x)$,  and sends it to $V$. From now on, $P$ can't change $t(x)$ without $V$ noticing. (For now you can see $[T]$ as a big number with special properties)
- `proof, t_z := open(t, z)`: the verifier still does not know anything about $t(x)$, but he wants to know $t(z)$. So the prover computes $t(z)$ and a `proof` to convince $V$.
- `verify(proof, t_z, [T])`: $V$ verifies that $t_z = t(z)$. The verifier can keep asking the prover to "reveal" the value of $t(x)$ at several points, and $P$ can't lie about them, they will always come from the same polynomial $t(x)$.

## Batch commitments
Usually we want to commit several polynomials, and open them up all at the same point. Most polynomial commitment schemes can be adapted to batch commit and batch open the polynomials, taking advantage of this fact to increase performance.

Example:
- `[T] := commit(t1, t2, ..., tn)`}
- `proof, t1_z, t2_z, ..., tn_z := open(t1, t2, ..., tn, z)`
- `verify(proof, t1_z, t2_z, ..., tn_z)`

## Trusted setup
Some polynomial schemes require a **trusted setup**. This is a ceremony that generate keys $V_k, P_k$ for the prover and the verifier. These keys are needed for the PCS to work. Generally, a **trusted** third party creates some secret $s$ and generates $V_k, P_k$ from $s$. Then $s$ **MUST** be thrown away and is called the **toxic waste**. 

Next we'll how arithmetization and PCS are used together to generate a ZK proof.

# Generating the proof
After arithmetization, both the prover and verifier have a system of polynomial constraints $R(x)$ that has the following property:

$$
R(\vec{w}) = 0 \iff \vec{w}\text{ is the execution of the agreed program}
$$

A good prover knows the correct $\vec{w}$, because it has executed the program. Suppose we encode $\vec{w}$ as a polynomial $w(x)$ such that $w(\omega^i)=\vec{w}_i$ via interpolation. We call this polynomial $w(x)$ the **witness** polynomial.  The prover does not want to reveal the witness to the verifier (it could have secret values). So $P$ must convince $V$ that he knows $w(x)$ such that $R(w(\omega^i))=0$ without revealing $w(x)$. Let's see the whole protocol and then understand how it works.

## Generic protocol
1. $P$ sends $[W]=\text{commit}(w)$ to $V$.
2. $P$ computes $H(x)$ such that $R(w(x))=H(x)Z(x)$ where $Z(x)=\prod_i(x-\omega^i)$.
3. $P$ sends $[H]=\text{commit}(H)$ to $V$.
4. $V$ sends a challenge $z$ to $V$.
5. $P$ opens $H(z), w(z)$ and sends the proofs and results to $V$.
6. $V$ verifies $H(z)$ and $w(z)$ using the PCS.
7. $V$ computes $R(w(z))$ and $Z(z)$.
8. $V$ checks that $R(w(z))=H(z)Z(z)$.

- Step 1: The prover commits to $w$, from now on it is fixed and $P$ cannot lie about the values $w(x)$ he sends to the prover.
- Step 2: Compute $H(x)$. the only reason $H(x)$ exists is because $Z(x)$ divides exactly $R(x)$. All the roots in $Z(x)$ are in $R(x)$, so $R(w(\omega^i))=0$ for all $i$.
- Step 3: $P$ commits to $H(x)$. It cannot change it anymore.
- Step 4: $V$ sends a challenge $z$. A **challenge** it's just a random field element, that $P$ cannot know in advance. This step is sometimes replaced by using the **Fiat-Shamir** heuristic to sample randomness. This makes the protocol non-interactive by letting $P$ sample randomness from the hash of the public input, commitments and interactions. The structure in charge of delivering this randomness is sometimes called the **transcript**. 
- Step 5: the prover computes $H(z), w(z)$ and sends them to the verifier. $P$ cannot lie about these values.
- Step 6: This step makes sure that $H(z)$ and $w(z)$ correspond to the commited $[H]$ and $[W]$. Note that this commitments were sent before knowing $z$.
- Step 7: Computes $R(x)$ at $w(z)$ and $Z(z)$. Here is where $V$ makes sure the proof is actually about the logic $R(x)$ of the agreed program.
- Step 8: Using the **Schwartz-Zippel** lemma $V$ convinces himself with overwhelming probability, that $R(w(x))=H(x)Z(x)$ (seen as polynomials) by just checking the equality at one random point $z$. This implies that $R(w(x))$ is zero over the domain $\omega^i$, that is, $w(x)$ must correspond to an execution of the program.

## Auxiliary arguments
There are some common "auxiliary" proofs that are helpful when designing proving systems.

### Permutation argument
The permutation argument is a zk proof on itself. The goal of this argument is to convince $V$ that two sets $S_1, S_2$ contain the same elements, without revealing its contents.

# ZK Ecosystem
There are many proving systems. They are very similar between each other. The main difference comes from the arithmetization and the PCS used. As mentioned before, the two main types of arithmetizations are those "based on circuits with gates" and those with an "execution trace".

## PCS
Next, there is a table with the most popular polynomial commitment schemes. **Transparent** refers to the trusted setup, FRI doesn't need trusted setup while KZG does. Succint refers to the succintness of the 

![image](https://hackmd.io/_uploads/rkJLroaua.png)

Minimum data:
- KZG: It is the most succinct of the PCS, but has a trusted setup. Uses elliptic curve cryptography to "hide" data inside the curve group.
- IPA: stands for "Inner product argument", it is a PCS built over another protocol that actually convinces $V$ that $P$ knows the solution of an inner product. U
- FRI: uses a mix of merkle trees and a low degree test.
https://eprint.iacr.org/2022/1216.pdf

## Protocols
Some popular protocols are:
- Groth16: first popular protocol. It is from 2016, it has arithmetization based on circuits and uses the KZG polynomial commitment scheme.
- Plonk: similar to groth16, also very popular. Uses circuits and KZG. Its advantage with respect to groth16 is that it encodes the witness more efficiently, and improves performance.
- STARK: can use any arithmetization, but uses the FRI protocol.

## Tools

# Use cases

# Further reading
